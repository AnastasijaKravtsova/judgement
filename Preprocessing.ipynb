{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для предобработки текстов приговоров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы импортировать функции из этой тетрадки(и из любой другой), нужно установить import-ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in /Users/vera/anaconda3/lib/python3.6/site-packages (0.1.3)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем важно, чтобы все нужные тетрадки лежали в одной директории. Потом запускаем такое: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! Теперь функции из этой тетрадки доступны в вашей тетрадке. Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приговоры в формате xml, чтобы работать с данными, нам нужно их распарсить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bound_pattern(patterns):\n",
    "    return re.compile('(?:{}).*\\\\n'.format('|'.join('\\\\s*'.join(x for x in s) for s in patterns)), re.IGNORECASE)\n",
    "\n",
    "begin_pattern = to_bound_pattern([\"УСТАНОВИЛ\"])\n",
    "end_pattern = to_bound_pattern([\"ПРИГОВОРИЛ\", \"ПОСТАНОВИЛ\"])\n",
    "\n",
    "def parse_xml(path):  # парсим xml\n",
    "\n",
    "    meta_data = {'court': '', 'judge': '', 'prosecutor': '', 'secretary': '', 'accused': '', 'result': '',\n",
    "                 'category': '', 'punishment_type': '', 'punishment_term': ''}  # складываем мета-данные в словарь\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(\"<soup>\" + f.read() + \"</soup>\", 'xml')\n",
    "\n",
    "        meta_data['court'] = soup.court.string  # добавляем в метаданные то, что имеется в разметке\n",
    "        meta_data['judge'] = soup.judge.string\n",
    "        meta_data['result'] = soup.result.string\n",
    "        meta_data['category'] = soup.category.string\n",
    "\n",
    "        html_soup = BeautifulSoup(soup.body.string, 'lxml')\n",
    "\n",
    "        return re.sub('\\\\n+', '\\\\n ', html_soup.get_text()), meta_data\n",
    "\n",
    "\n",
    "def get_parts(text):  # делим на части\n",
    "    \n",
    "    begin, main_part = re.split(begin_pattern, text, 1)\n",
    "    main_part, end = re.split(end_pattern, main_part, 1)\n",
    "    \n",
    "    return begin, main_part, end\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    pattern1 = re.compile(r'(п|ч|ст)(\\.|\\s|\\d)')\n",
    "    pattern2 = re.compile(r'((У|Г)П?К|КоАП|ПДД)\\sРФ')\n",
    "    pattern3 = re.compile('\\d')\n",
    "    pattern4 = re.compile('(ДД.ММ.ГГ|дд.мм.гг)')\n",
    "\n",
    "    text = re.sub(pattern1, ' ABBR ', text)\n",
    "    text = re.sub(pattern2, ' DOCUMENT ', text)\n",
    "    text = re.sub(pattern3, ' DIGIT ', text)\n",
    "    text = re.sub(pattern4, ' DATE ', text)\n",
    "    text = text.replace('\\xad', '')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    punct = punctuation+'«»—…“”*№–'\n",
    "    morph = MorphAnalyzer()\n",
    "    words = [word.strip(punct) for word in text.lower().split()] \n",
    "    words = [word for word in words if word]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать в отдельный модуль с заранее подготовленной train_data, и чтобы принимал только text\n",
    "def segmentate(train_data:str, text: str): #очень желательно, чтобы тренировочная выборка отличалась от той, на которую ее применяют.\n",
    "    \n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(train_data) #на случай, если тренировочные данные в формате списка, добавить ('\\n'.join(train_data))\n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    \n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    text, meta_data = parse_xml(path)\n",
    "    return tuple(map(clean, get_parts(text))), meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
