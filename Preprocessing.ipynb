{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для предобработки текстов приговоров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы импортировать функции из этой тетрадки(и из любой другой), нужно установить import-ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Running setup.py bdist_wheel for import-ipynb ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/vera/Library/Caches/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем важно, чтобы все нужные тетрадки лежали в одной директории. Потом запускаем такое: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from meta_extraction.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import meta_extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! Теперь функции из этой тетрадки доступны в вашей тетрадке. Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приговоры в формате xml, чтобы работать с данными, нам нужно их распарсить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import everygrams\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(path):   # парсим xml\n",
    "    \n",
    "    meta_data = {'court': '', 'judge': '', 'prosecutor': '', 'secretary': '', 'accused': '', 'result': '', 'category': '', \n",
    "            'punishment_type': '', 'punishment_term': ''} #складываем мета-данные в словарь\n",
    "    \n",
    "    with open(path, 'r', encoding='utf-8')as f:\n",
    "        \n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "        meta_data['court'] = soup.court.string     # добавляем в метаданные то, что имеется в разметке\n",
    "        meta_data['judge'] = soup.judge.string\n",
    "        meta_data['result'] = soup.result.string\n",
    "        meta_data['category'] = soup.category.string\n",
    "        \n",
    "        html = []\n",
    "        for line in soup.body:\n",
    "            line = line.replace('[', '')\n",
    "            line = line.replace(']', '') \n",
    "            html.append(line)\n",
    "        true_html = ' '.join(html)\n",
    "                \n",
    "        html_soup = BeautifulSoup(true_html, 'html.parser')\n",
    "        \n",
    "        return html_soup.get_text(), meta_data\n",
    "    \n",
    "    \n",
    "def get_parts(text):    # делим на части\n",
    "    \n",
    "    lines = [line for line in text.split('\\n')]\n",
    "    beg, end = 0, 0 \n",
    "    \n",
    "    for num, line in enumerate(lines):\n",
    "        \n",
    "        if re.search(r'у\\s*с\\s*т\\s*а\\s*н\\s*о\\s*в\\s*и\\s*л', line.lower()):\n",
    "            beg = num + 1\n",
    "            \n",
    "        if re.search(r'п\\s*р\\s*и\\s*г\\s*о\\s*в\\s*о\\s*р\\s*и\\s*л', line.lower()) \\\n",
    "            or re.search(r'п\\s*о\\s*с\\s*т\\s*а\\s*н\\s*о\\s*в\\s*и\\s*л', line.lower()):\n",
    "            \n",
    "            end = num + 1\n",
    "            \n",
    "        if beg and end:\n",
    "            \n",
    "            beginning = [stroka.strip() for stroka in lines[:beg]]\n",
    "            main_part = [stroka.strip() for stroka in lines[beg:end]]\n",
    "            ending = [stroka.strip() for stroka in lines[end:]]\n",
    "            \n",
    "            return [' '.join(main_part), ' '.join(beginning), ' '.join(ending)]\n",
    "        \n",
    "        \n",
    "def clean(text):\n",
    "    \n",
    "    pattern1 = re.compile(r'(п|ч|ст)(\\.|\\s|\\d)')\n",
    "    pattern2 = re.compile(r'((У|Г)П?К|КоАП|ПДД)\\sРФ')\n",
    "    pattern3 = re.compile('\\d')\n",
    "    pattern4 = re.compile('(ДД.ММ.ГГ|дд.мм.гг)')\n",
    "\n",
    "    text = re.sub(pattern1, ' ABBR ', text)\n",
    "    text = re.sub(pattern2, ' DOCUMENT ', text)\n",
    "    text = re.sub(pattern3, ' DIGIT ', text)\n",
    "    text = re.sub(pattern4, ' DATE ', text)\n",
    "    text = text.replace('\\xad', '')\n",
    "    text = text.replace('п р и г о в о р и л', '')\n",
    "    text = text.replace('п о с т а н о в и л', '')\n",
    "    text = text.replace('П Р И Г О В О Р И Л', '')\n",
    "    text = text.replace('П О С Т А Н О В И Л', '')\n",
    "    \n",
    "    return text\n",
    "        \n",
    "    \n",
    "def process_xml(path):\n",
    "    text, meta_data = parse_xml(path)\n",
    "    main_part = get_parts(text)[0]\n",
    "    cleaned_main_part = clean(main_part)\n",
    "    try:\n",
    "        return cleaned_main_part, meta_data\n",
    "    except TypeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    stops = set(stopwords.words('russian'))\n",
    "    punct = punctuation+'«»—…“”*№–'\n",
    "    morph = MorphAnalyzer()\n",
    "    words = [word.strip(punct) for word in text.lower().split()] \n",
    "    words = [word for word in words if word]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убрать в отдельный модуль с заранее подготовленной train_data, и чтобы принимал только text\n",
    "def segmentate(train_data:str, text: str): #очень желательно, чтобы тренировочная выборка отличалась от той, на которую ее применяют.\n",
    "    \n",
    "    trainer = PunktTrainer()\n",
    "    trainer.INCLUDE_ALL_COLLOCS = True\n",
    "    trainer.train(train_data) #на случай, если тренировочные данные в формате списка, добавить ('\\n'.join(train_data))\n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    \n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    text, meta_data = process_xml(path)\n",
    "    normalized_text = normalize(text)\n",
    "    \n",
    "    return normalized_text, meta_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
